##  Optimizerの説明

### SGD
SGD(Stochtic Gradient Descent:確率的勾配降下法)は、optimizarの中でも初期に提唱された最も基本的なアルゴリズム。重み
$w$の更新は以下の式のように行われる。この場合、 $E$は誤差関数、ηは学習係数を表している。

$$
\mathbf{w}^{t + 1} \gets \mathbf{w}^{t} - \eta \frac{\partial E(\mathbf{w}^{t})}{\partial \mathbf{w}^{t}}
$$

この手法は、通常の勾配法で起こりうる局所最適解への収束問題の解消を$w$の更新ごとにランダムにサンプルデータを選び出すことで解決した。
また、訓練データにおける冗長性を効率よく学習することができるという利点もある。
しかし、学習係数ηはハイパーパラメータであるため人間が恣意的に決定する必要があり、一度決めたηを用いて誤差の最小化を行なっていることになる。このことから、学習モデルによって最適なパラメータを決めることが難しいという問題点がある。

 >https://qiita.com/tokkuman/items/1944c00415d129ca0ee9
### Adam
Adam(Adaptive moment estimation)は2015年に提唱された手法で、AdaGradやRMSProp,AdaDeltaを改良したものである。  ハイパーパラメータの「バイアス補正（隔たりの補正）」が行われているということが特徴。  
重みの更新は以下の数式に従って行っていく。
パラメータは学習係数$α=0.001$、一次モーメント用の係数$β_1=0.9$、二次モーメント用の係数$β_2$=0.999、$ε=10^{-8}$が論文内で推奨されている。

$$
{m_{t+1} = \beta_{1} m_{t} + (1 - \beta_{1}) \nabla E(\mathbf{w}^{t})\\
v_{t+1} = \beta_{2} v_{t} + (1 - \beta_{2}) \nabla E(\mathbf{w}^{t})^{2}\\
\hat{m} = \frac{m_{t+1}}{1 - \beta_{1}^{t}}\\
\hat{v} = \frac{v_{t+1}}{1 - \beta_{2}^{t}}\\
\mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha \frac{\hat{m}}{\sqrt{\hat{v}} + \epsilon}
}
$$
初期値は $m_{0} = 0, v_{0} = 0$。




### Adagrad
学習係数は大きすぎると発散して学習不能に陥るが、逆に小さすぎると学習時間がかかってしまうという問題が発生してしまう。  
そのため、学習が進むにつれてパラメータ全体の学習係数を徐々にさげていくという学習係数の減衰（learning rate decoy）というアイデアを取り入れたものがAdagradである。  
言い換えると、Adagradは、パラメータの要素ごとに適応的に学習係数を調整しながら学習を行う手法である。  
更新方法の数式は以下の通りになる。  

 $$
{h_{0} = \epsilon\\
h_{t} = h_{t−１} + \nabla E(\mathbf{w}^{t})^{2}\\
\eta_{t} = \frac{\eta_{0}}{\sqrt{h_{t}}+\epsilon}\\
\mathbf{w}^{t+1} = \mathbf{w}^{t} - \eta_{t} \nabla E(\mathbf{w}^{t})
}
$$
